import OpenAI from 'openai';
// @ts-ignore
import * as fft from 'fft-js';
// @ts-ignore
import { Matrix } from 'ml-matrix';

/**
 * Audio Analysis Service - Uses OpenAI Whisper for comprehensive audio analysis
 */
export class AudioAnalysisService {
  private openai: OpenAI;

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY environment variable is required');
    }

    this.openai = new OpenAI({
      apiKey: apiKey,
    });
  }

  /**
   * Analyze audio from video URL or buffer
   */
  async analyzeAudio(audioBuffer: Buffer, options?: {
    detectMusic?: boolean;
    analyzeQuality?: boolean;
    detectCTAs?: boolean;
  }): Promise<{
    transcript: string;
    audioQuality: number;
    voiceStyle: string;
    musicDetected: boolean;
    musicType?: string;
    silencePercentage: number;
    speechRate: number;
    ctaAudio: string[];
    duration: number;
    processingTime: number;
    cost: number;
  }> {
    const startTime = Date.now();
    
    try {
      // Step 1: Transcription with Whisper
      // Create a proper File object for OpenAI SDK with correct MIME type
      const audioFile = new File([audioBuffer], 'audio.wav', { 
        type: 'audio/wav',
        lastModified: Date.now()
      });
      
      const transcriptionResponse = await this.openai.audio.transcriptions.create({
        file: audioFile,
        model: 'whisper-1',
        // Remove language to auto-detect the original language
        response_format: 'verbose_json',
        timestamp_granularities: ['word', 'segment']
      });

      // Step 2: Extract complete transcript with all timestamps
      const transcript = transcriptionResponse.text;
      const duration = transcriptionResponse.duration || 0;
      const language = (transcriptionResponse as any).language || 'unknown';
      const words = (transcriptionResponse as any).words || [];
      const segments = (transcriptionResponse as any).segments || [];
      
      console.log(`üéµ Transcription complete:`);
      console.log(`   Language detected: ${language}`);
      console.log(`   Total words: ${words.length}`);
      console.log(`   Total segments: ${segments.length}`);
      console.log(`   Duration: ${duration}s`);
      
      // Step 3: Analyze transcript content for insights
      const contentAnalysis = await this.analyzeTranscriptContent(transcript);
      
      // Step 4: Detect music and audio quality (using enhanced HPSS analysis with speech masking)
      const whisperData = { segments, words, language };
      const audioQualityAnalysis = await this.analyzeAudioQuality(transcript, duration, audioBuffer, whisperData);
      
      // Step 5: Calculate metrics
      const processingTime = Date.now() - startTime;
      const cost = this.calculateCost(duration);
      
      const result = {
        transcript,
        duration,
        processingTime,
        cost,
        ...contentAnalysis,
        ...audioQualityAnalysis
      };

      return result;
      
    } catch (error) {
      console.error('Audio analysis error:', error);
      throw new Error(`Audio analysis failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Analyze transcript content for marketing insights
   */
  private async analyzeTranscriptContent(transcript: string): Promise<{
    voiceStyle: string;
    speechRate: number;
    ctaAudio: string[];
  }> {
    if (!transcript.trim()) {
      return {
        voiceStyle: 'sem √°udio detectado',
        speechRate: 0,
        ctaAudio: []
      };
    }

    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: `Analise este texto transcrito de um √°udio publicit√°rio.
          
Retorne um JSON com:
1. voiceStyle: string - Descri√ß√£o do estilo da voz (ex: "energ√©tico, masculino, confi√°vel")
2. speechRate: number - Velocidade da fala (palavras por minuto estimado)  
3. ctaAudio: string[] - Calls-to-action identificados no √°udio

Seja preciso e objetivo.`
        },
        {
          role: 'user',
          content: `Transcri√ß√£o: "${transcript}"`
        }
      ],
      temperature: 0.3,
      max_tokens: 500,
      response_format: { type: "json_object" }
    });

    try {
      const analysis = JSON.parse(completion.choices[0].message.content || '{}');
      return {
        voiceStyle: analysis.voiceStyle || 'n√£o identificado',
        speechRate: analysis.speechRate || 0,
        ctaAudio: Array.isArray(analysis.ctaAudio) ? analysis.ctaAudio : []
      };
    } catch (error) {
      console.error('Error parsing transcript analysis:', error);
      return {
        voiceStyle: 'erro na an√°lise',
        speechRate: 0,
        ctaAudio: []
      };
    }
  }

  /**
   * Analyze audio quality and detect music with advanced heuristics and spectral analysis
   */
  private async analyzeAudioQuality(transcript: string, duration: number, audioBuffer?: Buffer, whisperData?: any): Promise<{
    audioQuality: number;
    musicDetected: boolean;
    musicType?: string;
    silencePercentage: number;
    voiceClarity: number;
    backgroundMusicPresence: number;
    spectralAnalysis?: any;
  }> {
    // Step 1: Enhanced HPSS spectral analysis if buffer is available
    let spectralAnalysis = null;
    let spectralInsights = '';
    
    if (audioBuffer) {
      // Pass transcript segments to enable speech masking in HPSS analysis
      const whisperTimestamps = whisperData?.segments || [];
      spectralAnalysis = await this.performSpectralAnalysis(audioBuffer, whisperTimestamps);
      
      // Include enhanced HPSS insights if analysis is valid
      if (spectralAnalysis.validAnalysis) {
        spectralInsights = `
An√°lise HPSS (Harmonic-Percussive Source Separation):
- M√∫sica de fundo detectada: ${spectralAnalysis.musicDetected ? 'SIM' : 'N√ÉO'} (confian√ßa: ${spectralAnalysis.confidence}/10)
- Clareza vocal: ${spectralAnalysis.voiceClarity}/10
- Presen√ßa harm√¥nica durante fala: ${(spectralAnalysis.musicEnergyScore/10).toFixed(2)}
- Periodicidade r√≠tmica: ${spectralAnalysis.bassPresence}/10
- An√°lise baseada em separa√ß√£o de fontes sonoras com m√°scara de fala`;
      } else {
        spectralInsights = `
An√°lise HPSS: DADOS INV√ÅLIDOS - assumindo conservadoramente SEM m√∫sica de fundo
- Qualidade do √°udio: N√£o foi poss√≠vel extrair dados PCM v√°lidos
- M√∫sica detectada: N√ÉO (fallback conservativo)
- Recomenda√ß√£o: Basear detec√ß√£o apenas na transcri√ß√£o`;
      }
    }

    // Step 2: Enhanced GPT-4o analysis with rich context
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: `Voc√™ √© um especialista em an√°lise de √°udio. Sua fun√ß√£o √© APENAS EXPLICAR e interpretar os resultados, N√ÉO detectar m√∫sica.

INSTRU√á√ïES CR√çTICAS PARA GATING HARD:
1. SEMPRE use o resultado "musicDetected" da an√°lise HPSS como fonte √öNICA de verdade
2. Se an√°lise HPSS detectou m√∫sica (musicDetected=true), NUNCA contradiga isso
3. Se an√°lise HPSS N√ÉO detectou m√∫sica (musicDetected=false), NUNCA sugira adicionar m√∫sica
4. Sua fun√ß√£o √© explicar os resultados t√©cnicos, n√£o fazer detec√ß√£o
5. Base musicType e musicGenre APENAS na transcri√ß√£o se musicDetected=true

GATING HARD: Voc√™ N√ÉO pode detectar m√∫sica. Use apenas o resultado HPSS.

Retorne JSON com:
{
  "audioQuality": number (1-10, baseado na clareza da transcri√ß√£o),
  "voiceClarity": number (1-10, clareza espec√≠fica da voz),
  "musicDetected": boolean (COPIE exatamente da an√°lise HPSS - N√ÉO modifique),
  "backgroundMusicPresence": number (1-10, COPIE da confian√ßa HPSS se musicDetected=true, sen√£o 0),
  "musicType": string (apenas se musicDetected=true, baseado na transcri√ß√£o),
  "musicGenre": string (apenas se musicDetected=true, baseado na transcri√ß√£o),
  "silencePercentage": number (0-100),
  "reasoning": string (explique que usou an√°lise HPSS como fonte √∫nica de verdade)
}`
        },
        {
          role: 'user',
          content: `DADOS PARA AN√ÅLISE:
Transcri√ß√£o: "${transcript}"
Dura√ß√£o: ${duration}s
Palavras na transcri√ß√£o: ${transcript.split(' ').length}
Densidade de fala: ${(transcript.split(' ').length / duration * 60).toFixed(1)} palavras/minuto
Tipo de conte√∫do: ${duration < 30 ? 'An√∫ncio curto' : duration < 60 ? 'An√∫ncio m√©dio' : 'Conte√∫do longo'}
${spectralInsights}

RESULTADO OBRIGAT√ìRIO DA AN√ÅLISE HPSS (fonte √∫nica de verdade):
- musicDetected: ${spectralAnalysis?.musicDetected || false}
- confidence: ${spectralAnalysis?.confidence || 0}/10

GATING HARD ATIVO: Use EXATAMENTE estes valores na sua resposta. N√ÉO modifique.

CONTEXTO DE AN√ÅLISE:
- A an√°lise HPSS √© baseada em separa√ß√£o de fontes harm√¥nicas/percussivas
- Usa m√°scara de fala para detectar m√∫sica durante per√≠odos de narra√ß√£o
- √â a fonte √öNICA e DEFINITIVA para detec√ß√£o de m√∫sica
- Sua fun√ß√£o √© apenas explicar e classificar, n√£o detectar`
        }
      ],
      temperature: 0.2, // Mais determin√≠stico para precis√£o
      max_tokens: 500,
      response_format: { type: "json_object" }
    });

    try {
      const analysis = JSON.parse(completion.choices[0].message.content || '{}');
      
      // GATING HARD: Use HPSS analysis as single source of truth
      let musicDetected = false;
      let musicConfidence = 0;
      
      // Primary evidence: HPSS analysis (single source of truth)
      if (spectralAnalysis && spectralAnalysis.validAnalysis) {
        musicDetected = spectralAnalysis.musicDetected;
        musicConfidence = spectralAnalysis.confidence;
        console.log(`üéµ HPSS Detection Result: ${musicDetected ? 'M√öSICA DETECTADA' : 'M√öSICA N√ÉO DETECTADA'} (confian√ßa: ${musicConfidence}/10)`);
      } else {
        // No valid spectral data - conservative fallback (no music)
        musicDetected = false;
        musicConfidence = 0;
        console.log(`üéµ HPSS Fallback: Dados inv√°lidos, assumindo M√öSICA N√ÉO DETECTADA`);
      }
      
      // Override GPT result with HPSS detection (gating hard)
      analysis.musicDetected = musicDetected;
      analysis.backgroundMusicPresence = musicConfidence;
      
      console.log(`üéµ Enhanced Audio Analysis Result:`);
      console.log(`   Music detected: ${analysis.musicDetected} (confidence: ${musicConfidence}/10)`);
      console.log(`   Voice clarity: ${analysis.voiceClarity}/10`);
      console.log(`   Audio quality: ${analysis.audioQuality}/10`);
      console.log(`   Reasoning: ${analysis.reasoning}`);
      
      // STRICT GATING HARD: HPSS is single source of truth, no overrides allowed
      const finalMusicDetected = spectralAnalysis?.validAnalysis ? spectralAnalysis.musicDetected : false;
      const finalMusicPresence = finalMusicDetected && spectralAnalysis ? Math.round(spectralAnalysis.confidence) : 0;
      
      return {
        audioQuality: Math.min(10, Math.max(1, analysis.audioQuality || 5)),
        voiceClarity: Math.min(10, Math.max(1, analysis.voiceClarity || 5)),
        musicDetected: finalMusicDetected, // STRICT: Only HPSS result, no confidence override
        backgroundMusicPresence: finalMusicPresence, // 0 if no music, confidence if music
        musicType: finalMusicDetected ? analysis.musicType : undefined,
        silencePercentage: Math.min(100, Math.max(0, analysis.silencePercentage || 0)),
        spectralAnalysis
      };
    } catch (error) {
      console.error('Error parsing enhanced audio quality analysis:', error);
      return {
        audioQuality: 5,
        voiceClarity: 5,
        musicDetected: false, // Conservative: no music when uncertain
        backgroundMusicPresence: 0, // Conservative: no music presence when uncertain
        silencePercentage: 20
      };
    }
  }

  /**
   * Calculate Whisper API cost
   */
  private calculateCost(durationSeconds: number): number {
    const durationMinutes = durationSeconds / 60;
    return durationMinutes * 0.003; // $0.003 per minute for Whisper
  }

  /**
   * Download and extract audio from video URL
   * FIXED: Now attempts proper audio extraction with robust validation
   */
  async extractAudioFromVideo(videoUrl: string): Promise<Buffer> {
    try {
      console.log(`üéµ Downloading video from: ${videoUrl}`);
      
      const response = await fetch(videoUrl, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
      });
      
      if (!response.ok) {
        throw new Error(`Failed to download video: ${response.status}`);
      }
      
      const videoBuffer = await response.arrayBuffer();
      const buffer = Buffer.from(videoBuffer);
      
      console.log(`üéµ Downloaded ${buffer.length} bytes of video data`);
      
      // Check if this is already audio format
      if (this.isValidAudioFormat(buffer)) {
        console.log(`‚úÖ Downloaded content is already in audio format`);
        return buffer;
      }
      
      // For video files, we need proper audio extraction
      // Since we don't have FFmpeg available, we'll validate and warn
      console.warn(`‚ö†Ô∏è Raw video buffer detected - audio extraction not implemented`);
      console.warn(`‚ö†Ô∏è Spectral analysis may produce invalid results`);
      
      // Return the buffer but mark it as potentially invalid for spectral analysis
      // The spectral analysis function will handle this gracefully
      return buffer;
      
    } catch (error) {
      console.error(`‚ùå Failed to extract audio from ${videoUrl}:`, error);
      throw new Error(`Failed to extract audio: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  // ========================================
  // SCENE-BASED AUDIO ALIGNMENT
  // ========================================

  /**
   * Analyze audio with detailed timestamps for scene alignment
   */
  async analyzeAudioWithTimestamps(audioBuffer: Buffer): Promise<{
    transcript: string;
    duration: number;
    words: Array<{
      word: string;
      start: number;
      end: number;
      confidence?: number;
    }>;
    segments: Array<{
      text: string;
      start: number;
      end: number;
      avgLogprob?: number;
      noSpeechProb?: number;
    }>;
    processingTime: number;
    cost: number;
  }> {
    const startTime = Date.now();
    
    try {
      // Create File object for Whisper API - Node.js compatible
      let audioFile: File;
      try {
        // Try Web File API (Node 18+) with correct WAV format
        audioFile = new File([audioBuffer], 'audio.wav', { 
          type: 'audio/wav',
          lastModified: Date.now()
        });
      } catch {
        // Fallback for older Node.js versions with correct WAV format
        const Blob = require('buffer').Blob || global.Blob;
        if (Blob) {
          audioFile = new Blob([audioBuffer], { type: 'audio/wav' }) as any;
          (audioFile as any).name = 'audio.wav';
        } else {
          // Last resort - create a minimal File-like object
          audioFile = Object.assign(audioBuffer, {
            name: 'audio.wav',
            type: 'audio/wav',
            size: audioBuffer.length,
            lastModified: Date.now()
          }) as any;
        }
      }
      
      // Use Whisper with auto language detection for original language transcription
      const transcriptionResponse = await this.openai.audio.transcriptions.create({
        file: audioFile,
        model: 'whisper-1',
        // No language specified - Whisper will auto-detect
        response_format: 'verbose_json',
        timestamp_granularities: ['word', 'segment']
      });
      
      const language = (transcriptionResponse as any).language || 'unknown';
      console.log(`üéµ Detected language: ${language}`);
      console.log(`üéµ Total words with timestamps: ${((transcriptionResponse as any).words || []).length}`);
      console.log(`üéµ Total segments: ${((transcriptionResponse as any).segments || []).length}`);

      const processingTime = Date.now() - startTime;
      const cost = this.calculateCost(transcriptionResponse.duration || 0);

      return {
        transcript: transcriptionResponse.text,
        duration: transcriptionResponse.duration || 0,
        words: (transcriptionResponse as any).words || [],
        segments: (transcriptionResponse as any).segments || [],
        processingTime,
        cost
      };
      
    } catch (error) {
      console.error('‚ùå Audio analysis with timestamps failed:', error);
      throw new Error(`Audio analysis failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Align transcript segments with scene timeline
   */
  async alignTranscriptWithScenes(
    transcriptData: {
      transcript: string;
      words: Array<{ word: string; start: number; end: number; confidence?: number }>;
      segments: Array<{ text: string; start: number; end: number }>;
    },
    sceneSegments: Array<{
      id: number;
      startSec: number;
      endSec: number;
      durationSec: number;
    }>,
    audioBuffer?: Buffer
  ): Promise<Array<{
    sceneId: number;
    startSec: number;
    endSec: number;
    audio: {
      transcriptSnippet: string;
      voicePresent: boolean;
      voiceStyle?: string;
      musicDetected: boolean;
      musicType?: string;
      soundEffects?: string[];
      audioQuality: number;
      volume: 'quiet' | 'normal' | 'loud';
      ctas?: string[];
    };
    wordsInScene: Array<{ word: string; start: number; end: number; confidence?: number }>;
    segmentsInScene: Array<{ text: string; start: number; end: number }>;
  }>> {
    console.log(`üéµ Aligning transcript with ${sceneSegments.length} scenes`);
    
    const alignedScenes = await Promise.all(sceneSegments.map(async scene => {
      // Find words that fall within this scene's time range (precise alignment)
      const wordsInScene = transcriptData.words.filter(word => {
        // Include word if its center point falls within the scene
        const wordCenter = (word.start + word.end) / 2;
        return wordCenter >= scene.startSec && wordCenter < scene.endSec;
      });

      // Find segments that overlap with this scene  
      const segmentsInScene = transcriptData.segments.filter(segment => {
        // Include segment if it overlaps with the scene
        return segment.start < scene.endSec && segment.end > scene.startSec;
      });

      // Extract transcript snippet for this scene
      const transcriptSnippet = segmentsInScene
        .map(segment => segment.text.trim())
        .join(' ')
        .trim();

      // Analyze audio characteristics for this scene using HPSS
      const audioAnalysis = await this.analyzeSceneAudio(
        transcriptSnippet, 
        wordsInScene, 
        scene, 
        audioBuffer,
        segmentsInScene  // CRITICAL FIX: Use filtered segments for this scene, not global segments
      );

      console.log(`üéµ Scene ${scene.id}: ${wordsInScene.length} words, ${segmentsInScene.length} segments`);
      
      // Log first and last word timestamps for validation
      if (wordsInScene.length > 0) {
        console.log(`   First word: "${wordsInScene[0].word}" at ${wordsInScene[0].start.toFixed(2)}s`);
        console.log(`   Last word: "${wordsInScene[wordsInScene.length - 1].word}" at ${wordsInScene[wordsInScene.length - 1].end.toFixed(2)}s`);
      }

      return {
        sceneId: scene.id,
        startSec: scene.startSec,
        endSec: scene.endSec,
        audio: audioAnalysis,
        wordsInScene,
        segmentsInScene
      };
    }));

    // Validation: Check coverage and completeness
    const totalWordsInScenes = alignedScenes.reduce((sum, scene) => sum + scene.wordsInScene.length, 0);
    const coveragePercentage = (totalWordsInScenes / transcriptData.words.length) * 100;
    
    console.log(`‚úÖ Audio alignment complete for all scenes`);
    console.log(`üìä Transcript coverage: ${totalWordsInScenes}/${transcriptData.words.length} words (${coveragePercentage.toFixed(1)}%)`);
    
    // Warn if coverage is less than 95%
    if (coveragePercentage < 95) {
      console.warn(`‚ö†Ô∏è Low transcript coverage: ${coveragePercentage.toFixed(1)}%. Some words may be missing between scenes.`);
    }
    
    return alignedScenes;
  }

  /**
   * Check if two time ranges overlap
   */
  private hasTimeOverlap(
    segment: { start: number; end: number },
    scene: { startSec: number; endSec: number }
  ): boolean {
    return segment.start < scene.endSec && segment.end > scene.startSec;
  }

  /**
   * Analyze audio characteristics for a specific scene with HPSS-based music detection
   */
  private async analyzeSceneAudio(
    transcriptSnippet: string,
    words: Array<{ word: string; start: number; end: number; confidence?: number }>,
    scene: { startSec: number; endSec: number; durationSec: number },
    audioBuffer?: Buffer,
    whisperSegments?: any[]
  ) {
    const voicePresent = transcriptSnippet.length > 0;
    
    // Calculate speech rate (words per minute)
    const speechRate = voicePresent ? (words.length / scene.durationSec) * 60 : 0;
    
    // Detect CTAs in transcript - expanded list
    const ctaKeywords = [
      'compre', 'comprar', 'adquira', 'garanta', 'aproveite', 'clique', 'acesse',
      'visite', 'ligue', 'contate', 'cadastre-se', 'inscreva-se', 'baixe',
      'download', 'oferta', 'desconto', 'promo√ß√£o', 'agora', 'hoje',
      '√∫ltimos dias', 'por tempo limitado', 'pe√ßa j√°', 'solicite',
      'loja', 'site', 'whatsapp', 'telefone', 'encomende', 'reserve'
    ];
    
    const ctas = ctaKeywords.filter(keyword => 
      transcriptSnippet.toLowerCase().includes(keyword.toLowerCase())
    );

    // Advanced voice style analysis based on transcript content and tone
    let voiceStyle: string | undefined;
    const content = transcriptSnippet.toLowerCase();
    const exclamationCount = (transcriptSnippet.match(/!/g) || []).length;
    const questionCount = (transcriptSnippet.match(/\?/g) || []).length;
    
    if (voicePresent) {
      
      if (exclamationCount > 1 || content.includes('incr√≠vel') || content.includes('fant√°stico') || content.includes('uau')) {
        voiceStyle = 'energetic';
      } else if (content.includes('profissional') || content.includes('qualidade') || content.includes('especialista')) {
        voiceStyle = 'professional';
      } else if (content.includes('voc√™') || content.includes('seu') || content.includes('sua')) {
        voiceStyle = 'conversational';
      } else if (questionCount > 0 || content.includes('problema') || content.includes('solu√ß√£o')) {
        voiceStyle = 'problem-solving';
      } else if (content.includes('exclusivo') || content.includes('limitado') || content.includes('especial')) {
        voiceStyle = 'urgent';
      } else {
        voiceStyle = 'neutral';
      }
    }

    // Enhanced audio quality estimation
    const avgConfidence = words.length > 0 
      ? words.reduce((sum, word) => sum + (word.confidence || 0.8), 0) / words.length
      : 0.5;
    
    // Base quality calculation with better scaling
    let audioQuality = 5; // Default neutral
    if (words.length > 0) {
      // Good speech recognition = higher quality
      audioQuality = Math.max(6, Math.round(avgConfidence * 10));
      
      // Boost for clear, well-paced speech
      if (speechRate > 100 && speechRate < 160 && words.length > 5) {
        audioQuality = Math.min(10, audioQuality + 1);
      }
    } else {
      // No speech detected - could be music-only or poor quality
      audioQuality = scene.durationSec > 5 ? 4 : 2; // Longer silence suggests intentional
    }

    // Calculate speech density (words per second)
    const speechDensity = voicePresent ? (words.length / scene.durationSec) : 0;

    // Enhanced volume estimation based on multiple factors
    let volume: 'quiet' | 'normal' | 'loud';
    if (speechRate < 80 || words.length < 3) {
      volume = 'quiet';
    } else if (speechRate > 200 || exclamationCount > 2) {
      volume = 'loud';
    } else {
      volume = 'normal';
    }

    // HPSS-BASED MUSIC DETECTION - Replace heuristics with spectral analysis
    let musicDetected = false;
    let musicType: string | undefined;
    let musicConfidence = 0;
    let spectralAnalysis = null;
    
    // Use HPSS pipeline for precise music detection if audio buffer is available
    if (audioBuffer && scene.durationSec > 1) {
      try {
        console.log(`üî¨ Running HPSS analysis for scene ${scene.startSec}-${scene.endSec}s`);
        
        // Extract audio segment for this scene
        const sceneAudioBuffer = await this.extractSceneAudioBuffer(audioBuffer, scene.startSec, scene.endSec);
        
        // Filter whisper segments for this scene
        const sceneWhisperData = whisperSegments?.filter(segment => 
          segment.start >= scene.startSec && segment.end <= scene.endSec
        ) || [];
        
        // Perform HPSS spectral analysis on scene audio with time window
        spectralAnalysis = await this.performSpectralAnalysis(sceneAudioBuffer, sceneWhisperData, scene.startSec, scene.endSec);
        
        musicDetected = spectralAnalysis.musicDetected;
        musicConfidence = spectralAnalysis.confidence;
        
        console.log(`üéµ HPSS Scene ${scene.startSec}-${scene.endSec}s: Music detected: ${musicDetected}, confidence: ${musicConfidence}/10`);
        
        // Determine music type based on spectral characteristics and context
        if (musicDetected) {
          if (spectralAnalysis.harmonicContent > 7 && spectralAnalysis.bassPresence > 6) {
            musicType = 'upbeat';
          } else if (spectralAnalysis.harmonicContent > 8) {
            musicType = 'instrumental';
          } else if (voicePresent && spectralAnalysis.harmonicContent > 5) {
            musicType = 'background';
          } else {
            musicType = 'ambient';
          }
        }
        
      } catch (error) {
        console.warn(`‚ö†Ô∏è HPSS analysis failed for scene ${scene.startSec}-${scene.endSec}s:`, error);
        // Fallback to conservative detection
        musicDetected = false;
        musicConfidence = 0;
      }
    } else {
      console.log(`üéµ Scene ${scene.startSec}-${scene.endSec}s: No audio buffer provided, skipping HPSS analysis`);
      musicDetected = false;
      musicConfidence = 0;
    }

    return {
      transcriptSnippet,
      voicePresent,
      voiceStyle,
      musicDetected,
      musicType,
      musicConfidence: Math.min(10, musicConfidence),
      soundEffects: [], // Would need advanced audio analysis
      audioQuality,
      volume,
      ctas: ctas.length > 0 ? ctas : undefined,
      speechMetrics: {
        speechRate,
        wordCount: words.length,
        speechDensity: Math.round(speechDensity),
        avgConfidence: Math.round(avgConfidence * 100) / 100
      }
    };
  }

  /**
   * Extract audio buffer segment for a specific scene
   */
  private async extractSceneAudioBuffer(
    audioBuffer: Buffer, 
    startSec: number, 
    endSec: number
  ): Promise<Buffer> {
    try {
      // For now, we'll return the full audio buffer since scene-specific extraction
      // requires more complex audio processing. The HPSS analysis with timestamps
      // will focus on the relevant time range.
      return audioBuffer;
    } catch (error) {
      console.warn(`‚ö†Ô∏è Failed to extract scene audio buffer: ${error}`);
      return audioBuffer;
    }
  }

  /**
   * Calculate audio-visual sync quality for scenes
   */
  calculateSyncQuality(
    alignedScenes: Array<{
      sceneId: number;
      audio: { transcriptSnippet: string; voicePresent: boolean; ctas?: string[] };
    }>,
    visualScenes: Array<{
      id: number;
      text: Array<{ content: string }>;
      objects: Array<{ label: string }>;
      brandElements: string[];
    }>
  ): Array<{
    sceneId: number;
    syncQuality: number;
    syncAnalysis: {
      audioVisualMatch: number;
      ctaTextAlignment: number;
      brandMention: number;
    };
  }> {
    return alignedScenes.map(audioScene => {
      const visualScene = visualScenes.find(v => v.id === audioScene.sceneId);
      
      if (!visualScene) {
        return {
          sceneId: audioScene.sceneId,
          syncQuality: 3,
          syncAnalysis: {
            audioVisualMatch: 3,
            ctaTextAlignment: 3,
            brandMention: 3
          }
        };
      }

      // Check if audio matches visual text
      const visualTexts = visualScene.text.map(t => t.content.toLowerCase());
      const audioText = audioScene.audio.transcriptSnippet.toLowerCase();
      
      let audioVisualMatch = 5; // Default neutral
      for (const visualText of visualTexts) {
        if (audioText.includes(visualText) || visualText.includes(audioText.substring(0, 20))) {
          audioVisualMatch = Math.min(10, audioVisualMatch + 2);
        }
      }

      // Check CTA alignment
      const ctaTextAlignment = audioScene.audio.ctas && audioScene.audio.ctas.length > 0 
        ? (visualTexts.some(vt => audioScene.audio.ctas!.some(cta => vt.includes(cta))) ? 9 : 6)
        : 5;

      // Check brand mention alignment
      const brandMention = visualScene.brandElements.length > 0 && audioScene.audio.voicePresent
        ? (visualScene.brandElements.some(brand => audioText.includes(brand.toLowerCase())) ? 8 : 4)
        : 5;

      const syncQuality = Math.round((audioVisualMatch + ctaTextAlignment + brandMention) / 3);

      return {
        sceneId: audioScene.sceneId,
        syncQuality,
        syncAnalysis: {
          audioVisualMatch,
          ctaTextAlignment,
          brandMention
        }
      };
    });
  }

  /**
   * Validate if file is a valid audio/video format
   */
  isValidAudioFormat(buffer: Buffer): boolean {
    // Check magic numbers for common audio/video formats
    const magic = buffer.slice(0, 4);
    
    // MP3
    if (magic[0] === 0xFF && (magic[1] & 0xE0) === 0xE0) return true;
    
    // MP4/M4A
    if (buffer.slice(4, 8).toString() === 'ftyp') return true;
    
    // WAV
    if (magic.toString() === 'RIFF') return true;
    
    // WebM
    if (magic.slice(0, 4).toString('hex') === '1a45dfa3') return true;
    
    return false;
  }

  // ========================================
  // ADVANCED SPECTRAL ANALYSIS
  // ========================================

  /**
   * Perform HPSS-based analysis to distinguish music from voice with speech masking
   * @param audioBuffer - Audio buffer to analyze
   * @param whisperTimestamps - Whisper segments for speech masking
   * @param startSec - Start time in seconds (optional, for scene analysis)
   * @param endSec - End time in seconds (optional, for scene analysis)
   */
  private async performSpectralAnalysis(
    audioBuffer: Buffer, 
    whisperTimestamps?: any[], 
    startSec?: number, 
    endSec?: number
  ): Promise<{
    musicEnergyScore: number;
    voiceClarity: number;
    bassPresence: number;
    harmonicContent: number;
    dynamicRange: number;
    spectralCentroid: number;
    musicLikelihood: number;
    frequencyDistribution: any;
    validAnalysis: boolean;
    musicDetected: boolean;
    confidence: number;
  }> {
    try {
      const timeWindow = startSec !== undefined && endSec !== undefined ? `${startSec}-${endSec}s` : 'full audio';
      console.log(`üî¨ Starting HPSS-based spectral analysis of ${audioBuffer.length} bytes (${timeWindow})`);
      
      // Validate if this is proper audio data
      if (!this.isValidAudioFormat(audioBuffer)) {
        console.warn(`‚ö†Ô∏è Invalid audio format detected - returning conservative estimates`);
        return this.getConservativeSpectralAnalysisEnhanced();
      }
      
      // Extract and preprocess PCM samples
      let samples = this.extractAndPreprocessPCM(audioBuffer);
      const sampleRate = 16000; // Standardized sample rate
      
      // Apply time window filtering if specified (for scene analysis)
      if (startSec !== undefined && endSec !== undefined) {
        const startSample = Math.floor(startSec * sampleRate);
        const endSample = Math.floor(endSec * sampleRate);
        const windowSamples = endSample - startSample;
        
        if (startSample < samples.length && endSample <= samples.length && windowSamples > 0) {
          samples = samples.slice(startSample, endSample);
          console.log(`üî¨ Applied time window ${startSec}-${endSec}s: ${windowSamples} samples`);
        } else {
          console.warn(`‚ö†Ô∏è Invalid time window ${startSec}-${endSec}s for ${samples.length} samples, using full audio`);
        }
      }
      
      // Validate extracted samples
      if (samples.length === 0 || samples.every(s => s === 0)) {
        console.warn(`‚ö†Ô∏è No valid PCM samples extracted - returning conservative estimates`);
        return this.getConservativeSpectralAnalysisEnhanced();
      }
      
      // Convert to log-mel spectrogram
      const melSpectrogram = this.computeLogMelSpectrogram(samples, sampleRate);
      
      // Apply HPSS (Harmonic-Percussive Source Separation)
      const { harmonic, percussive } = this.applyHPSS(melSpectrogram);
      
      // Validate HPSS results
      if (harmonic.length === 0 || percussive.length === 0) {
        console.warn(`‚ö†Ô∏è HPSS separation failed - returning conservative estimates`);
        return this.getConservativeSpectralAnalysisEnhanced();
      }
      
      // Create speech mask from Whisper timestamps
      const speechMask = this.createSpeechMask(whisperTimestamps || [], samples.length, sampleRate);
      
      // Analyze music features during speech periods
      const musicAnalysis = this.analyzeMusicDuringSpeech(harmonic, percussive, speechMask, sampleRate);
      
      console.log(`üéµ HPSS Analysis Results:`);
      console.log(`   Music Detected: ${musicAnalysis.musicDetected}`);
      console.log(`   Confidence: ${musicAnalysis.confidence}/10`);
      console.log(`   Harmonic Ratio During Speech: ${musicAnalysis.harmonicRatioSpeech.toFixed(3)}`);
      console.log(`   Beat Periodicity: ${musicAnalysis.beatPeriodicity.toFixed(3)}`);
      console.log(`   Speech Coverage: ${musicAnalysis.speechCoverage.toFixed(1)}%`);
      
      return {
        musicEnergyScore: musicAnalysis.harmonicRatioSpeech * 10,
        voiceClarity: musicAnalysis.voiceClarity,
        bassPresence: musicAnalysis.bassPresence,
        harmonicContent: musicAnalysis.harmonicRatio * 10,
        dynamicRange: musicAnalysis.dynamicRange,
        spectralCentroid: musicAnalysis.spectralCentroid,
        musicLikelihood: musicAnalysis.confidence,
        frequencyDistribution: musicAnalysis.freqDistribution,
        validAnalysis: true,
        musicDetected: musicAnalysis.musicDetected,
        confidence: musicAnalysis.confidence
      };
      
    } catch (error) {
      console.error('üî¨ HPSS analysis failed:', error);
      return this.getConservativeSpectralAnalysisEnhanced();
    }
  }

  /**
   * Extract and preprocess PCM samples with RMS normalization and filtering
   */
  /**
   * Extract and preprocess PCM samples with robust error handling
   * FIXED: Added validation and better error handling
   */
  private extractAndPreprocessPCM(audioBuffer: Buffer): number[] {
    const samples: number[] = [];
    
    try {
      // Validate buffer size
      if (audioBuffer.length < 44) {
        console.warn('üî¨ Buffer too small for WAV header, using fallback');
        return this.generateFallbackSamples();
      }
      
      // Simple 16-bit PCM extraction (assumes WAV format)
      for (let i = 44; i < audioBuffer.length - 1; i += 2) { // Skip WAV header
        try {
          const sample = audioBuffer.readInt16LE(i) / 32768.0; // Normalize to [-1, 1]
          if (!isNaN(sample) && isFinite(sample)) {
            samples.push(Math.max(-1, Math.min(1, sample))); // Clamp to valid range
          }
        } catch (error) {
          // Skip invalid samples
          continue;
        }
      }
      
      if (samples.length === 0) {
        console.warn('üî¨ No valid PCM samples extracted, using fallback');
        return this.generateFallbackSamples();
      }
      
      // Convert to mono if needed and downsample to 16kHz
      const processed = this.downsampleTo16kHz(samples);
      
      // Apply RMS normalization with validation
      const normalized = this.applyRMSNormalization(processed);
      
      // Apply high-pass filter (50 Hz) to remove low-frequency noise
      const filtered = this.applyHighPassFilter(normalized, 16000, 50);
      
      // Validate final samples
      const validSamples = filtered.filter(sample => !isNaN(sample) && isFinite(sample));
      
      if (validSamples.length === 0) {
        console.warn('üî¨ All processed samples invalid, using fallback');
        return this.generateFallbackSamples();
      }
      
      console.log(`üî¨ Extracted and preprocessed ${validSamples.length} PCM samples (16kHz)`);
      return validSamples.slice(0, Math.min(validSamples.length, 16000 * 30)); // Max 30 seconds
      
    } catch (error) {
      console.error('üî¨ PCM extraction failed:', error);
      return this.generateFallbackSamples();
    }
  }

  /**
   * Apply Hamming window to reduce spectral leakage
   */
  private applyHammingWindow(frame: number[]): number[] {
    const N = frame.length;
    return frame.map((sample, i) => {
      const windowValue = 0.54 - 0.46 * Math.cos((2 * Math.PI * i) / (N - 1));
      return sample * windowValue;
    });
  }

  /**
   * Analyze spectral features to distinguish music from voice
   */
  private analyzeSpectralFeatures(spectralFrames: number[][], sampleRate: number): {
    musicEnergyScore: number;
    voiceClarity: number;
    bassPresence: number;
    harmonicContent: number;
    dynamicRange: number;
    spectralCentroid: number;
    musicLikelihood: number;
    frequencyDistribution: any;
    validAnalysis: boolean;
  } {
    const nyquist = sampleRate / 2;
    const frameCount = spectralFrames.length;
    const binCount = spectralFrames[0].length;
    
    // Define frequency bands
    const bassRange = [60, 250];      // Bass frequencies (typical in music)
    const voiceRange = [300, 3400];   // Human voice fundamental range
    const presenceRange = [2000, 5000]; // Voice presence and clarity
    const trebleRange = [5000, 12000]; // High frequencies (music instruments)
    
    let bassEnergy = 0;
    let voiceEnergy = 0;
    let presenceEnergy = 0;
    let trebleEnergy = 0;
    let totalEnergy = 0;
    let spectralCentroidSum = 0;
    
    const energyOverTime: number[] = [];
    
    for (const frame of spectralFrames) {
      let frameEnergy = 0;
      let weightedFreqSum = 0;
      let frameBasEnergy = 0;
      let frameVoiceEnergy = 0;
      let framePresenceEnergy = 0;
      let frameTrebleEnergy = 0;
      
      for (let bin = 0; bin < binCount; bin++) {
        const frequency = (bin * nyquist) / binCount;
        const magnitude = frame[bin];
        const energy = magnitude * magnitude;
        
        frameEnergy += energy;
        weightedFreqSum += frequency * energy;
        
        // Classify energy by frequency bands
        if (frequency >= bassRange[0] && frequency <= bassRange[1]) {
          frameBasEnergy += energy;
        }
        if (frequency >= voiceRange[0] && frequency <= voiceRange[1]) {
          frameVoiceEnergy += energy;
        }
        if (frequency >= presenceRange[0] && frequency <= presenceRange[1]) {
          framePresenceEnergy += energy;
        }
        if (frequency >= trebleRange[0] && frequency <= trebleRange[1]) {
          frameTrebleEnergy += energy;
        }
      }
      
      energyOverTime.push(frameEnergy);
      bassEnergy += frameBasEnergy;
      voiceEnergy += frameVoiceEnergy;
      presenceEnergy += framePresenceEnergy;
      trebleEnergy += frameTrebleEnergy;
      totalEnergy += frameEnergy;
      
      // Calculate spectral centroid for this frame
      if (frameEnergy > 0) {
        spectralCentroidSum += weightedFreqSum / frameEnergy;
      }
    }
    
    // Calculate averages
    const avgSpectralCentroid = spectralCentroidSum / frameCount;
    
    // Normalize energy distributions
    const normalizedBass = totalEnergy > 0 ? (bassEnergy / totalEnergy) * 100 : 0;
    const normalizedVoice = totalEnergy > 0 ? (voiceEnergy / totalEnergy) * 100 : 0;
    const normalizedPresence = totalEnergy > 0 ? (presenceEnergy / totalEnergy) * 100 : 0;
    const normalizedTreble = totalEnergy > 0 ? (trebleEnergy / totalEnergy) * 100 : 0;
    
    // Calculate dynamic range (variation in energy over time)
    const energyVariance = this.calculateVariance(energyOverTime);
    const dynamicRange = Math.min(10, Math.sqrt(energyVariance) * 3);
    
    // Score calculations (1-10 scale)
    
    // Bass presence indicates music (especially in commercial content)
    const bassPresence = Math.min(10, Math.max(1, normalizedBass * 0.4));
    
    // Voice clarity based on energy in voice frequency range
    const voiceClarity = Math.min(10, Math.max(1, normalizedVoice * 0.3 + normalizedPresence * 0.2));
    
    // Harmonic content - music tends to have more complex harmonics
    const harmonicContent = Math.min(10, Math.max(1, 
      (normalizedTreble * 0.1) + (dynamicRange * 0.3) + (bassPresence * 0.2) + 3
    ));
    
    // Music energy score - combined indicators
    const musicEnergyScore = Math.min(10, Math.max(1,
      (bassPresence * 0.3) + 
      (harmonicContent * 0.2) + 
      (dynamicRange * 0.2) + 
      (normalizedTreble * 0.1) +
      ((avgSpectralCentroid > 800 && avgSpectralCentroid < 2000) ? 2 : 0) // Sweet spot for music
    ));
    
    // Overall music likelihood
    const musicLikelihood = Math.min(10, Math.max(1,
      (musicEnergyScore * 0.4) + 
      (bassPresence * 0.3) + 
      (harmonicContent * 0.3)
    ));
    
    return {
      musicEnergyScore: Math.round(musicEnergyScore),
      voiceClarity: Math.round(voiceClarity), 
      bassPresence: Math.round(bassPresence),
      harmonicContent: Math.round(harmonicContent),
      dynamicRange: Math.round(dynamicRange),
      spectralCentroid: Math.round(avgSpectralCentroid),
      musicLikelihood: Math.round(musicLikelihood),
      frequencyDistribution: {
        bass: Math.round(normalizedBass),
        voice: Math.round(normalizedVoice),
        presence: Math.round(normalizedPresence),
        treble: Math.round(normalizedTreble)
      },
      validAnalysis: true
    };
  }

  /**
   * Return conservative spectral analysis when audio data is invalid
   * ADDED: Safe fallback for invalid audio data
   */
  private getConservativeSpectralAnalysis(): {
    musicEnergyScore: number;
    voiceClarity: number;
    bassPresence: number;
    harmonicContent: number;
    dynamicRange: number;
    spectralCentroid: number;
    musicLikelihood: number;
    frequencyDistribution: any;
    validAnalysis: boolean;
  } {
    return {
      musicEnergyScore: 3, // Conservative, avoid false positives
      voiceClarity: 5, // Neutral assumption
      bassPresence: 2, // Low confidence without real data
      harmonicContent: 3, // Conservative
      dynamicRange: 4, // Moderate assumption
      spectralCentroid: 1000, // Typical speech range
      musicLikelihood: 3, // Conservative - lean toward no music
      frequencyDistribution: {
        bass: 10,
        voice: 60,
        presence: 25,
        treble: 5
      },
      validAnalysis: false // Mark as invalid data
    };
  }

  /**
   * Calculate variance of an array of numbers
   */
  private calculateVariance(values: number[]): number {
    if (values.length === 0) return 0;
    
    const mean = values.reduce((sum, val) => sum + val, 0) / values.length;
    const variance = values.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / values.length;
    
    return variance;
  }

  // ========================================
  // HPSS (HARMONIC-PERCUSSIVE SOURCE SEPARATION) METHODS
  // ========================================

  /**
   * Enhanced conservative spectral analysis with musicDetected flag
   */
  private getConservativeSpectralAnalysisEnhanced(): {
    musicEnergyScore: number;
    voiceClarity: number;
    bassPresence: number;
    harmonicContent: number;
    dynamicRange: number;
    spectralCentroid: number;
    musicLikelihood: number;
    frequencyDistribution: any;
    validAnalysis: boolean;
    musicDetected: boolean;
    confidence: number;
  } {
    return {
      musicEnergyScore: 2, // Conservative - assume minimal music
      voiceClarity: 6,
      bassPresence: 1,
      harmonicContent: 2,
      dynamicRange: 3,
      spectralCentroid: 1500,
      musicLikelihood: 2, // Low confidence
      frequencyDistribution: {
        bass: 10,
        voice: 60,
        presence: 25,
        treble: 5
      },
      validAnalysis: false,
      musicDetected: false, // Conservative: assume no music when unsure
      confidence: 2
    };
  }

  /**
   * Downsample audio to 16kHz
   */
  private downsampleTo16kHz(samples: number[]): number[] {
    // Simple decimation (in practice, you'd use proper anti-aliasing)
    const targetRate = 16000;
    const originalRate = 44100; // Assume original is 44.1kHz
    const ratio = Math.floor(originalRate / targetRate);
    
    const downsampled: number[] = [];
    for (let i = 0; i < samples.length; i += ratio) {
      downsampled.push(samples[i]);
    }
    
    return downsampled;
  }

  /**
   * Generate fallback samples for analysis when extraction fails
   * ADDED: Safe fallback for invalid audio data
   */
  private generateFallbackSamples(): number[] {
    const samples: number[] = [];
    // Generate minimal noise for analysis
    for (let i = 0; i < 16000; i++) { // 1 second at 16kHz
      samples.push(Math.random() * 0.01 - 0.005); // Very small noise
    }
    console.log('üî¨ Generated fallback samples for analysis');
    return samples;
  }

  /**
   * Apply RMS normalization with validation
   * FIXED: Added validation for edge cases
   */
  private applyRMSNormalization(samples: number[]): number[] {
    if (samples.length === 0) return samples;
    
    try {
      const validSamples = samples.filter(sample => !isNaN(sample) && isFinite(sample));
      if (validSamples.length === 0) return samples;
      
      const rms = Math.sqrt(validSamples.reduce((sum, sample) => sum + sample * sample, 0) / validSamples.length);
      const targetRMS = 0.1; // Target RMS level
      
      if (rms === 0 || !isFinite(rms)) return samples;
      
      const gain = targetRMS / rms;
      return samples.map(sample => {
        if (!isNaN(sample) && isFinite(sample)) {
          return Math.max(-1, Math.min(1, sample * gain));
        }
        return 0; // Replace invalid samples with silence
      });
    } catch (error) {
      console.warn('‚ö†Ô∏è RMS normalization failed:', error);
      return samples;
    }
  }

  /**
   * Calculate magnitude from FFT result manually
   * ADDED: Helper method to replace fft.util.fftMag
   */
  private calculateMagnitude(fftResult: any[]): number[] {
    const magnitude: number[] = [];
    
    try {
      for (let i = 0; i < fftResult.length; i++) {
        const real = fftResult[i][0] || 0;
        const imag = fftResult[i][1] || 0;
        magnitude.push(Math.sqrt(real * real + imag * imag));
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è Magnitude calculation failed:', error);
      // Return zeros if calculation fails
      return new Array(fftResult.length).fill(0);
    }
    
    return magnitude;
  }

  /**
   * Apply simple high-pass filter
   */
  private applyHighPassFilter(samples: number[], sampleRate: number, cutoffHz: number): number[] {
    // Simple first-order high-pass filter
    const RC = 1.0 / (2 * Math.PI * cutoffHz);
    const dt = 1.0 / sampleRate;
    const alpha = RC / (RC + dt);
    
    const filtered: number[] = [];
    let prevInput = 0;
    let prevOutput = 0;
    
    for (const sample of samples) {
      const output = alpha * (prevOutput + sample - prevInput);
      filtered.push(output);
      prevInput = sample;
      prevOutput = output;
    }
    
    return filtered;
  }

  /**
   * Compute log-mel spectrogram (simplified)
   * FIXED: Corrected fft-js API usage and added error handling
   */
  private computeLogMelSpectrogram(samples: number[], sampleRate: number): number[][] {
    const windowSize = 1024;
    const hopSize = windowSize / 2;
    const melBins = 80;
    
    const spectrogram: number[][] = [];
    
    try {
      for (let i = 0; i < samples.length - windowSize; i += hopSize) {
        const frame = samples.slice(i, i + windowSize);
        const windowed = this.applyHammingWindow(frame);
        
        // FIXED: Correct fft-js API usage
        const fftResult = fft.fft(windowed);
        
        // Calculate magnitude manually if fft.util.fftMag is not available
        const magnitude = this.calculateMagnitude(fftResult);
        
        // Convert to mel scale (simplified)
        const melFrame = this.convertToMelScale(magnitude, sampleRate, melBins);
        
        // Apply log
        const logMelFrame = melFrame.map(val => Math.log(Math.max(val, 1e-10)));
        spectrogram.push(logMelFrame);
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è FFT computation failed:', error);
      // Return empty spectrogram on error
      return [];
    }
    
    return spectrogram;
  }

  /**
   * Convert magnitude spectrum to mel scale (simplified)
   */
  private convertToMelScale(magnitude: number[], sampleRate: number, melBins: number): number[] {
    // Simplified mel-scale conversion
    const melScale = new Array(melBins).fill(0);
    const binRatio = magnitude.length / melBins;
    
    for (let i = 0; i < melBins; i++) {
      const startBin = Math.floor(i * binRatio);
      const endBin = Math.floor((i + 1) * binRatio);
      
      let energy = 0;
      for (let j = startBin; j < endBin && j < magnitude.length; j++) {
        energy += magnitude[j];
      }
      melScale[i] = energy / (endBin - startBin);
    }
    
    return melScale;
  }


  /**
   * Apply HPSS (Harmonic-Percussive Source Separation)
   * FIXED: Added validation for empty spectrogram
   */
  private applyHPSS(spectrogram: number[][]): { harmonic: number[][], percussive: number[][] } {
    if (spectrogram.length === 0 || spectrogram[0].length === 0) {
      console.warn('‚ö†Ô∏è Empty spectrogram provided to HPSS');
      return { harmonic: [], percussive: [] };
    }
    
    const rows = spectrogram.length;
    const cols = spectrogram[0].length;
    
    // Create harmonic and percussive components
    const harmonic: number[][] = [];
    const percussive: number[][] = [];
    
    // Apply median filtering for separation
    for (let i = 0; i < rows; i++) {
      harmonic[i] = [];
      percussive[i] = [];
      
      for (let j = 0; j < cols; j++) {
        // Harmonic component: median filter across time (horizontal)
        const timeSlice = [];
        for (let t = Math.max(0, i - 2); t <= Math.min(rows - 1, i + 2); t++) {
          timeSlice.push(spectrogram[t][j]);
        }
        const harmonicVal = this.median(timeSlice);
        
        // Percussive component: median filter across frequency (vertical)
        const freqSlice = [];
        for (let f = Math.max(0, j - 2); f <= Math.min(cols - 1, j + 2); f++) {
          freqSlice.push(spectrogram[i][f]);
        }
        const percussiveVal = this.median(freqSlice);
        
        // Soft masking
        const total = harmonicVal + percussiveVal;
        if (total > 0) {
          harmonic[i][j] = (harmonicVal / total) * spectrogram[i][j];
          percussive[i][j] = (percussiveVal / total) * spectrogram[i][j];
        } else {
          harmonic[i][j] = 0;
          percussive[i][j] = 0;
        }
      }
    }
    
    return { harmonic, percussive };
  }

  /**
   * Calculate median of array
   */
  private median(arr: number[]): number {
    const sorted = [...arr].sort((a, b) => a - b);
    const mid = Math.floor(sorted.length / 2);
    return sorted.length % 2 !== 0 ? sorted[mid] : (sorted[mid - 1] + sorted[mid]) / 2;
  }

  /**
   * Create speech mask from Whisper timestamps
   */
  private createSpeechMask(timestamps: any[], audioLength: number, sampleRate: number): boolean[] {
    const mask = new Array(Math.floor(audioLength / (sampleRate * 0.025))).fill(false); // 25ms frames
    
    for (const segment of timestamps) {
      if (segment.start !== undefined && segment.end !== undefined) {
        const startFrame = Math.floor(segment.start / 0.025);
        const endFrame = Math.floor(segment.end / 0.025);
        
        for (let i = startFrame; i < Math.min(endFrame, mask.length); i++) {
          mask[i] = true;
        }
      }
    }
    
    return mask;
  }

  /**
   * Analyze music features during speech periods
   */
  private analyzeMusicDuringSpeech(harmonic: number[][], percussive: number[][], speechMask: boolean[], sampleRate: number): {
    musicDetected: boolean;
    confidence: number;
    harmonicRatioSpeech: number;
    beatPeriodicity: number;
    speechCoverage: number;
    voiceClarity: number;
    bassPresence: number;
    harmonicRatio: number;
    dynamicRange: number;
    spectralCentroid: number;
    freqDistribution: any;
  } {
    // Calculate harmonic energy during speech
    let harmonicEnergySpeech = 0;
    let totalEnergySpeech = 0;
    let speechFrames = 0;
    
    const minFrames = Math.min(harmonic.length, speechMask.length);
    
    for (let i = 0; i < minFrames; i++) {
      if (speechMask[i]) {
        speechFrames++;
        
        const harmonicFrame = harmonic[i].reduce((sum, val) => sum + val, 0);
        const percussiveFrame = percussive[i].reduce((sum, val) => sum + val, 0);
        const totalFrame = harmonicFrame + percussiveFrame;
        
        harmonicEnergySpeech += harmonicFrame;
        totalEnergySpeech += totalFrame;
      }
    }
    
    const speechCoverage = (speechFrames / minFrames) * 100;
    const harmonicRatioSpeech = totalEnergySpeech > 0 ? harmonicEnergySpeech / totalEnergySpeech : 0;
    
    // Calculate beat periodicity (simplified)
    const beatPeriodicity = this.calculateBeatPeriodicity(percussive);
    
    // Calculate overall harmonic ratio
    let totalHarmonic = 0;
    let totalPercussive = 0;
    
    for (const frame of harmonic) {
      totalHarmonic += frame.reduce((sum, val) => sum + val, 0);
    }
    for (const frame of percussive) {
      totalPercussive += frame.reduce((sum, val) => sum + val, 0);
    }
    
    const harmonicRatio = totalHarmonic / (totalHarmonic + totalPercussive);
    
    // Music detection logic based on architectural guidance
    const musicDetected = (
      (harmonicRatioSpeech > 0.20 && beatPeriodicity > 0.20 && speechCoverage >= 40) ||
      (harmonicRatio > 0.35 && beatPeriodicity > 0.15)
    );
    
    // Calculate confidence
    let confidence = 0;
    if (musicDetected) {
      confidence = Math.min(10, (harmonicRatioSpeech * 15) + (beatPeriodicity * 10) + (harmonicRatio * 5));
    } else {
      confidence = Math.max(0, 10 - (harmonicRatioSpeech * 15) - (beatPeriodicity * 10));
    }
    
    return {
      musicDetected,
      confidence: Math.round(confidence),
      harmonicRatioSpeech,
      beatPeriodicity,
      speechCoverage,
      voiceClarity: speechCoverage >= 60 ? 8 : 5,
      bassPresence: beatPeriodicity * 10,
      harmonicRatio,
      dynamicRange: this.calculateDynamicRange(harmonic),
      spectralCentroid: this.calculateSpectralCentroid(harmonic),
      freqDistribution: {
        bass: Math.round(beatPeriodicity * 40),
        voice: Math.round(speechCoverage * 0.6),
        presence: Math.round(harmonicRatioSpeech * 50),
        treble: Math.round(harmonicRatio * 30)
      }
    };
  }

  /**
   * Calculate beat periodicity (simplified)
   */
  private calculateBeatPeriodicity(percussive: number[][]): number {
    if (percussive.length === 0) return 0;
    
    // Sum percussive energy across frequency bins for each frame
    const onsetStrength = percussive.map(frame => 
      frame.reduce((sum, val) => sum + val, 0)
    );
    
    // Calculate onset variations (simplified beat detection)
    let variations = 0;
    for (let i = 1; i < onsetStrength.length; i++) {
      const diff = Math.abs(onsetStrength[i] - onsetStrength[i - 1]);
      if (diff > 0.1) variations++;
    }
    
    return Math.min(1, variations / onsetStrength.length * 10);
  }

  /**
   * Calculate dynamic range from harmonic component
   */
  private calculateDynamicRange(harmonic: number[][]): number {
    const energies = harmonic.map(frame => frame.reduce((sum, val) => sum + val, 0));
    const max = Math.max(...energies);
    const min = Math.min(...energies.filter(e => e > 0));
    
    if (min === 0 || max === 0) return 0;
    
    return Math.min(10, Math.log10(max / min));
  }

  /**
   * Calculate spectral centroid
   */
  private calculateSpectralCentroid(harmonic: number[][]): number {
    if (harmonic.length === 0) return 0;
    
    let weightedSum = 0;
    let totalEnergy = 0;
    
    for (const frame of harmonic) {
      for (let i = 0; i < frame.length; i++) {
        weightedSum += i * frame[i];
        totalEnergy += frame[i];
      }
    }
    
    return totalEnergy > 0 ? (weightedSum / totalEnergy) * 100 : 0; // Normalize to reasonable range
  }

  /**
   * Create standardized File object for Whisper API
   * FIXED: Consistent file handling across all methods
   */
  private createWhisperFile(audioBuffer: Buffer): File {
    try {
      // Try Web File API (Node 18+) with correct WAV format
      return new File([audioBuffer], 'audio.wav', { 
        type: 'audio/wav',
        lastModified: Date.now()
      });
    } catch {
      // Fallback for older Node.js versions
      const Blob = require('buffer').Blob || global.Blob;
      if (Blob) {
        const file = new Blob([audioBuffer], { type: 'audio/wav' }) as any;
        file.name = 'audio.wav';
        return file;
      } else {
        // Last resort - create a minimal File-like object
        return Object.assign(audioBuffer, {
          name: 'audio.wav',
          type: 'audio/wav',
          size: audioBuffer.length,
          lastModified: Date.now()
        }) as any;
      }
    }
  }
}