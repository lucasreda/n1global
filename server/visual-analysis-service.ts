import OpenAI from 'openai';

/**
 * Visual Analysis Service - Uses GPT-4o Vision for comprehensive visual analysis
 */
export class VisualAnalysisService {
  private openai: OpenAI;

  // Pricing per 1K tokens for cost calculation
  private modelPricing = {
    'gpt-4o': { input: 0.005, output: 0.015 }
  };

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY environment variable is required');
    }

    this.openai = new OpenAI({
      apiKey: apiKey,
    });
  }

  /**
   * Analyze keyframes for comprehensive visual insights
   */
  async analyzeKeyframes(keyframes: Array<{
    timestamp: number;
    url: string;
    base64?: string;
    description: string;
    frameType: string;
    isPlaceholder?: boolean;
  }>, options?: {
    includeProducts?: boolean;
    detectText?: boolean;
    analyzeComposition?: boolean;
    brandAnalysis?: boolean;
  }): Promise<{
    keyframes: Array<{
      timestamp: number;
      url: string;
      description: string;
      objects: string[];
      text: string[];
      visualScore: number;
      insights: string[];
    }>;
    products: string[];
    people: number;
    logoVisibility: number;
    textOnScreen: string[];
    colors: string[];
    composition: string;
    visualQuality: number;
    processingTime: number;
    cost: number;
  }> {
    const startTime = Date.now();
    
    try {
      // Step 1: Analyze each keyframe individually
      const analyzedKeyframes = await this.analyzeIndividualFrames(keyframes);
      
      // Step 2: Perform comprehensive analysis across all frames
      const overallAnalysis = await this.analyzeOverallVisualPattern(analyzedKeyframes);
      
      // Step 3: Calculate processing time and cost
      const processingTime = Date.now() - startTime;
      const cost = this.calculateCost(keyframes.length);
      
      return {
        keyframes: analyzedKeyframes,
        processingTime,
        cost,
        ...overallAnalysis
      };
      
    } catch (error) {
      console.error('Visual analysis error:', error);
      throw new Error(`Visual analysis failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Analyze individual keyframes
   */
  private async analyzeIndividualFrames(keyframes: Array<{
    timestamp: number;
    url: string;
    base64?: string;
    description: string;
    frameType: string;
    isPlaceholder?: boolean;
  }>): Promise<Array<{
    timestamp: number;
    url: string;
    description: string;
    objects: string[];
    text: string[];
    visualScore: number;
    insights: string[];
  }>> {
    const analyzedFrames = [];

    for (const keyframe of keyframes) {
      // Skip placeholder keyframes to avoid sending invalid data to GPT-4o
      if (keyframe.isPlaceholder || !keyframe.url) {
        console.log(`‚ö†Ô∏è Skipping placeholder keyframe at ${keyframe.timestamp}s`);
        analyzedFrames.push({
          timestamp: keyframe.timestamp,
          url: keyframe.url,
          description: keyframe.description + ' (placeholder - n√£o analisado)',
          objects: [],
          text: [],
          visualScore: 1, // Very low score for placeholders
          insights: ['Keyframe placeholder - an√°lise visual n√£o dispon√≠vel']
        });
        continue;
      }

      const analysis = await this.analyzeKeyframe(keyframe);
      analyzedFrames.push({
        timestamp: keyframe.timestamp,
        url: keyframe.url,
        description: keyframe.description,
        ...analysis
      });
    }

    return analyzedFrames;
  }

  /**
   * Analyze single keyframe
   */
  private async analyzeKeyframe(keyframe: {
    timestamp: number;
    url: string;
    base64?: string;
    frameType: string;
  }): Promise<{
    objects: string[];
    text: string[];
    visualScore: number;
    insights: string[];
  }> {
    try {
      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: `Analise esta imagem de um criativo publicit√°rio aos ${keyframe.timestamp}s.

Retorne um JSON com:
1. objects: string[] - Objetos principais identificados (produtos, pessoas, logos, etc.)
2. text: string[] - Textos vis√≠veis na imagem (headlines, CTAs, pre√ßos)
3. visualScore: number - Qualidade visual de 1-10 (composi√ß√£o, clareza, impacto)
4. insights: string[] - 3-5 insights espec√≠ficos sobre a qualidade publicit√°ria

Seja preciso e focado em elementos que impactam performance publicit√°ria.`
          },
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: `Frame tipo: ${keyframe.frameType} aos ${keyframe.timestamp}s`
              },
              {
                type: 'image_url',
                image_url: {
                  url: keyframe.url,
                  detail: 'high'
                }
              }
            ]
          }
        ],
        temperature: 0.3,
        max_tokens: 800,
        response_format: { type: "json_object" }
      });

      const analysis = JSON.parse(completion.choices[0].message.content || '{}');
      
      return {
        objects: Array.isArray(analysis.objects) ? analysis.objects : [],
        text: Array.isArray(analysis.text) ? analysis.text : [],
        visualScore: Math.min(10, Math.max(1, analysis.visualScore || 5)),
        insights: Array.isArray(analysis.insights) ? analysis.insights : []
      };

    } catch (error) {
      console.error('Keyframe analysis error:', error);
      return {
        objects: [],
        text: [],
        visualScore: 5,
        insights: ['Erro na an√°lise visual']
      };
    }
  }

  /**
   * Analyze overall visual patterns across all keyframes
   */
  private async analyzeOverallVisualPattern(keyframes: Array<{
    timestamp: number;
    objects: string[];
    text: string[];
    visualScore: number;
    insights: string[];
  }>): Promise<{
    products: string[];
    people: number;
    logoVisibility: number;
    textOnScreen: string[];
    colors: string[];
    composition: string;
    visualQuality: number;
  }> {
    try {
      // Extract data from all keyframes
      const allObjects = keyframes.flatMap(k => k.objects);
      const allText = keyframes.flatMap(k => k.text);
      const avgVisualScore = keyframes.reduce((sum, k) => sum + k.visualScore, 0) / keyframes.length;
      
      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: `Analise este conjunto de keyframes de um v√≠deo publicit√°rio.

Baseado nos dados extra√≠dos, retorne um JSON com:
1. products: string[] - Produtos identificados ao longo do v√≠deo
2. people: number - N√∫mero estimado de pessoas aparecendo
3. logoVisibility: number - Visibilidade do logo de 1-10
4. textOnScreen: string[] - Principais textos que aparecem na tela
5. colors: string[] - Cores dominantes identificadas
6. composition: string - Descri√ß√£o da composi√ß√£o visual geral
7. visualQuality: number - Qualidade visual geral de 1-10`
          },
          {
            role: 'user',
            content: `Dados dos keyframes:
            
Objetos encontrados: ${allObjects.join(', ')}
Textos encontrados: ${allText.join(', ')}
Score visual m√©dio: ${avgVisualScore.toFixed(1)}
N√∫mero de keyframes: ${keyframes.length}

Timeline dos keyframes: ${keyframes.map(k => `${k.timestamp}s`).join(', ')}`
          }
        ],
        temperature: 0.3,
        max_tokens: 600,
        response_format: { type: "json_object" }
      });

      const analysis = JSON.parse(completion.choices[0].message.content || '{}');
      
      return {
        products: Array.isArray(analysis.products) ? analysis.products : [],
        people: Math.max(0, analysis.people || 0),
        logoVisibility: Math.min(10, Math.max(1, analysis.logoVisibility || 5)),
        textOnScreen: Array.isArray(analysis.textOnScreen) ? analysis.textOnScreen : [],
        colors: Array.isArray(analysis.colors) ? analysis.colors : [],
        composition: analysis.composition || 'Composi√ß√£o padr√£o',
        visualQuality: Math.min(10, Math.max(1, analysis.visualQuality || avgVisualScore))
      };

    } catch (error) {
      console.error('Overall pattern analysis error:', error);
      return {
        products: [],
        people: 0,
        logoVisibility: 5,
        textOnScreen: [],
        colors: [],
        composition: 'An√°lise n√£o dispon√≠vel',
        visualQuality: 5
      };
    }
  }

  /**
   * Analyze single image (for image creatives)
   */
  async analyzeImage(imageUrl: string): Promise<{
    objects: string[];
    text: string[];
    composition: string;
    visualQuality: number;
    products: string[];
    logoVisibility: number;
    colors: string[];
    insights: string[];
    processingTime: number;
    cost: number;
  }> {
    const startTime = Date.now();
    
    try {
      // Download image and convert to data URL to avoid Facebook CDN expiration issues
      let finalImageUrl = imageUrl;
      
      if (imageUrl.includes('facebook.com') || imageUrl.includes('fbcdn.net')) {
        console.log(`üîç DEBUG: Downloading Facebook image to avoid CDN expiration: ${imageUrl}`);
        try {
          const imageResponse = await fetch(imageUrl);
          if (imageResponse.ok) {
            const imageBuffer = await imageResponse.arrayBuffer();
            const base64 = Buffer.from(imageBuffer).toString('base64');
            
            // Determine image type from response headers or URL
            const contentType = imageResponse.headers.get('content-type') || 'image/png';
            finalImageUrl = `data:${contentType};base64,${base64}`;
            console.log(`‚úÖ DEBUG: Facebook image downloaded and converted to data URL (${imageBuffer.byteLength} bytes)`);
          } else {
            console.warn(`‚ö†Ô∏è DEBUG: Failed to download Facebook image (${imageResponse.status}), using original URL`);
          }
        } catch (downloadError) {
          console.warn(`‚ö†Ô∏è DEBUG: Error downloading Facebook image:`, downloadError);
          console.warn(`‚ö†Ô∏è DEBUG: Falling back to original URL`);
        }
      }
      
      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: `Analise esta imagem publicit√°ria de forma abrangente.

Retorne um JSON com:
1. objects: string[] - Objetos identificados
2. text: string[] - Textos vis√≠veis
3. composition: string - An√°lise da composi√ß√£o visual
4. visualQuality: number - Qualidade de 1-10
5. products: string[] - Produtos identificados
6. logoVisibility: number - Visibilidade da marca de 1-10
7. colors: string[] - Cores dominantes
8. insights: string[] - Insights para otimiza√ß√£o publicit√°ria`
          },
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: 'Analise esta imagem publicit√°ria:'
              },
              {
                type: 'image_url',
                image_url: {
                  url: finalImageUrl,
                  detail: 'high'
                }
              }
            ]
          }
        ],
        temperature: 0.3,
        max_tokens: 1000,
        response_format: { type: "json_object" }
      });

      const analysis = JSON.parse(completion.choices[0].message.content || '{}');
      const processingTime = Date.now() - startTime;
      const cost = this.modelPricing['gpt-4o'].input * 0.5; // Estimate for single image
      
      return {
        objects: Array.isArray(analysis.objects) ? analysis.objects : [],
        text: Array.isArray(analysis.text) ? analysis.text : [],
        composition: analysis.composition || '',
        visualQuality: Math.min(10, Math.max(1, analysis.visualQuality || 5)),
        products: Array.isArray(analysis.products) ? analysis.products : [],
        logoVisibility: Math.min(10, Math.max(1, analysis.logoVisibility || 5)),
        colors: Array.isArray(analysis.colors) ? analysis.colors : [],
        insights: Array.isArray(analysis.insights) ? analysis.insights : [],
        processingTime,
        cost
      };

    } catch (error) {
      console.error('Image analysis error:', error);
      throw new Error(`Image analysis failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Calculate estimated cost for visual analysis
   */
  private calculateCost(keyframeCount: number): number {
    // Estimate tokens per keyframe analysis
    const avgTokensPerFrame = 800; // Input tokens for image + prompt
    const totalInputTokens = keyframeCount * avgTokensPerFrame;
    const totalOutputTokens = keyframeCount * 200; // Estimated output tokens
    
    const inputCost = (totalInputTokens / 1000) * this.modelPricing['gpt-4o'].input;
    const outputCost = (totalOutputTokens / 1000) * this.modelPricing['gpt-4o'].output;
    
    return inputCost + outputCost;
  }

  // ========================================
  // SCENE-BY-SCENE TECHNICAL ANALYSIS
  // ========================================

  /**
   * Analyze scenes with technical precision (Google Vision alternative)
   */
  async analyzeScenes(sceneSegments: Array<{
    id: number;
    startSec: number;
    endSec: number;
    durationSec: number;
    keyframes: Array<{
      timestamp: number;
      url: string;
      tempPath?: string;
    }>;
  }>, audioScenes?: Array<{
    id: number;
    startSec: number;
    endSec: number;
    transcriptSnippet: string;
    voicePresent: boolean;
    musicDetected: boolean;
    speechRate: number;
    ctas: string[];
  }>): Promise<Array<{
    id: number;
    startSec: number;
    endSec: number;
    durationSec: number;
    technicalDescription: string;
    objects: Array<{
      label: string;
      count: number;
      confidence?: number;
      boundingBox?: { x: number; y: number; width: number; height: number };
    }>;
    text: Array<{
      content: string;
      position?: string;
      fontSize?: string;
      fontStyle?: string;
      color?: string;
    }>;
    peopleCount: number;
    dominantColors: string[];
    brandElements: string[];
    composition: {
      shotType: string;
      cameraMovement: string;
      cameraAngle: string;
      lighting: string;
      depth: string;
    };
    transitionIn?: { type: string; duration: number; effect?: string };
    transitionOut?: { type: string; duration: number; effect?: string };
    motionIntensity: number;
    visualComplexity: number;
    visualScore: number;
    engagementScore: number;
    keyframes: Array<{
      timestamp: number;
      url: string;
      description: string;
    }>;
  }>> {
    console.log(`üé¨ Starting technical scene analysis for ${sceneSegments.length} scenes`);
    
    const analyzedScenes = [];
    
    for (const scene of sceneSegments) {
      if (scene.keyframes.length === 0) {
        console.warn(`‚ö†Ô∏è Scene ${scene.id} has no keyframes, skipping`);
        continue;
      }

      console.log(`üîç Analyzing scene ${scene.id} (${scene.startSec.toFixed(2)}s-${scene.endSec.toFixed(2)}s) with ${scene.keyframes.length} keyframes`);
      
      try {
        // Find corresponding audio data for this scene (try ID match first, then time overlap)
        let audioContext = audioScenes?.find(audioScene => audioScene.id === scene.id);
        
        // Fallback: match by time overlap if ID doesn't match
        if (!audioContext && audioScenes) {
          audioContext = audioScenes.find(audioScene => 
            audioScene.startSec <= scene.endSec && audioScene.endSec >= scene.startSec
          );
        }
        
        if (audioContext) {
          console.log(`üéµ Scene ${scene.id} matched with audio context: "${audioContext.transcriptSnippet?.substring(0, 50)}..."`);
        } else {
          console.log(`‚ö†Ô∏è Scene ${scene.id} has no audio context available`);
        }
        
        const sceneAnalysis = await this.performTechnicalSceneAnalysis(scene, audioContext);
        analyzedScenes.push(sceneAnalysis);
        
        console.log(`‚úÖ Scene ${scene.id} analysis complete - ${sceneAnalysis.objects.length} objects, ${sceneAnalysis.text.length} text elements detected`);
      } catch (error) {
        console.error(`‚ùå Failed to analyze scene ${scene.id}:`, error);
        
        // Fallback analysis for failed scenes
        analyzedScenes.push(this.createFallbackSceneAnalysis(scene));
      }
    }

    console.log(`‚úÖ Technical scene analysis complete: ${analyzedScenes.length} scenes processed`);
    return analyzedScenes;
  }

  /**
   * Perform detailed technical analysis for a single scene using multiple keyframes
   */
  private async performTechnicalSceneAnalysis(scene: {
    id: number;
    startSec: number;
    endSec: number;
    durationSec: number;
    keyframes: Array<{
      timestamp: number;
      url: string;
    }>;
  }, audioContext?: {
    transcriptSnippet: string;
    voicePresent: boolean;
    musicDetected: boolean;
    speechRate: number;
    ctas: string[];
  }): Promise<any> {
    try {
      // Prepare image content for GPT-4o Vision
      const imageContent = scene.keyframes.map((keyframe, index) => ({
        type: 'image_url' as const,
        image_url: {
          url: keyframe.url,
          detail: 'high' as const
        }
      }));

      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o',
        max_tokens: 2000,
        temperature: 0.2, // Low temperature for consistent technical analysis
        response_format: { type: "json_object" },
        messages: [
          {
            role: 'system',
            content: `Voc√™ √© um especialista em an√°lise audiovisual t√©cnica, equivalente ao Google Vision API + an√°lise de contexto. Analise esta cena de v√≠deo publicit√°rio com m√°xima precis√£o t√©cnica integrando elementos visuais e auditivos.

INSTRU√á√ïES CR√çTICAS:
- Analise TODAS as imagens em conjunto como uma √∫nica cena temporal
- INTEGRE o contexto de √°udio/narra√ß√£o para criar descri√ß√µes mais precisas e espec√≠ficas
- Use a transcri√ß√£o para entender o que est√° acontecendo e enriquecer a descri√ß√£o t√©cnica
- Conecte elementos visuais com o que est√° sendo dito ou a m√∫sica presente
- Forne√ßa descri√ß√£o t√©cnica detalhada n√≠vel profissional que considere AMBOS visual e √°udio
- Identifique TODOS os objetos, pessoas, textos, cores com precis√£o
- Analise composi√ß√£o cinematogr√°fica, ilumina√ß√£o, movimento de c√¢mera
- Detecte transi√ß√µes e mudan√ßas visuais entre frames
- Retorne JSON estruturado v√°lido com todos os campos obrigat√≥rios

CAMPOS OBRIGAT√ìRIOS NO JSON:
{
  "technicalDescription": "string - Descri√ß√£o t√©cnica completa e contextualizada da cena, integrando elementos visuais (composi√ß√£o, ilumina√ß√£o, movimento, a√ß√£o, produtos, pessoas, textos vis√≠veis, cores) com contexto auditivo (narra√ß√£o, m√∫sica, CTAs) para uma an√°lise precisa e espec√≠fica",
  "objects": [{"label": "string", "count": number, "confidence": number}],
  "text": [{"content": "string", "position": "string", "fontSize": "string", "color": "string"}],
  "peopleCount": number,
  "dominantColors": ["#hex"],
  "brandElements": ["string"],
  "composition": {
    "shotType": "close-up|medium|wide|extreme-wide",
    "cameraMovement": "static|pan|tilt|zoom-in|zoom-out|dolly",
    "cameraAngle": "eye-level|high-angle|low-angle|dutch-angle",
    "lighting": "natural|artificial|mixed|dramatic|soft|harsh",
    "depth": "shallow|deep|medium"
  },
  "transitionIn": {"type": "cut|fade|dissolve", "duration": number, "effect": "string"},
  "transitionOut": {"type": "cut|fade|dissolve", "duration": number, "effect": "string"},
  "motionIntensity": number,
  "visualComplexity": number,
  "visualScore": number,
  "engagementScore": number
}`
          },
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: `An√°lise t√©cnica contextualizada da Cena ${scene.id}:

INFORMA√á√ïES TEMPORAIS:
- Dura√ß√£o: ${scene.startSec.toFixed(2)}s a ${scene.endSec.toFixed(2)}s (${scene.durationSec.toFixed(2)}s total)
- Keyframes: ${scene.keyframes.length} imagens sequenciais

CONTEXTO DE √ÅUDIO${audioContext ? ':' : ' (n√£o dispon√≠vel):'}${audioContext ? `
- Transcri√ß√£o: "${audioContext.transcriptSnippet}"
- Voz presente: ${audioContext.voicePresent ? 'Sim' : 'N√£o'}
- M√∫sica: ${audioContext.musicDetected ? 'Detectada' : 'N√£o detectada'}
- Velocidade da fala: ${audioContext.speechRate} palavras/min
- CTAs identificados: ${audioContext.ctas.length > 0 ? audioContext.ctas.join(', ') : 'Nenhum'}

IMPORTANTE: Use essas informa√ß√µes de √°udio para enriquecer sua descri√ß√£o t√©cnica visual. Conecte o que est√° sendo dito com o que est√° sendo mostrado visualmente.` : `
- Sem dados de √°udio dispon√≠veis para esta cena
IMPORTANTE: Foque exclusivamente na an√°lise visual t√©cnica detalhada.`}

INSTRU√á√ïES ESPECIAIS:
${audioContext?.transcriptSnippet ? 
`- Use a transcri√ß√£o "${audioContext.transcriptSnippet}" para contextualizar o que est√° acontecendo visualmente
- Conecte elementos visuais com o que est√° sendo narrado
- Identifique como a narrativa se alinha com os elementos visuais mostrados` :
`- Foque apenas na an√°lise visual, mas seja extremamente detalhado`}

Forne√ßa an√°lise t√©cnica completa e contextualizada:`
              },
              ...imageContent
            ]
          }
        ]
      });

      const analysisText = completion.choices[0]?.message?.content;
      if (!analysisText) {
        throw new Error('Empty response from GPT-4o');
      }

      let analysisData;
      try {
        analysisData = JSON.parse(analysisText);
      } catch (parseError) {
        console.warn(`‚ö†Ô∏è Failed to parse JSON, using text analysis for scene ${scene.id}`);
        throw new Error(`Invalid JSON response: ${parseError}`);
      }

      // Validate and structure the response
      return {
        id: scene.id,
        startSec: scene.startSec,
        endSec: scene.endSec,
        durationSec: scene.durationSec,
        technicalDescription: analysisData.technicalDescription || 'An√°lise t√©cnica n√£o dispon√≠vel',
        objects: Array.isArray(analysisData.objects) ? analysisData.objects : [],
        text: Array.isArray(analysisData.text) ? analysisData.text : [],
        peopleCount: typeof analysisData.peopleCount === 'number' ? analysisData.peopleCount : 0,
        dominantColors: Array.isArray(analysisData.dominantColors) ? analysisData.dominantColors : [],
        brandElements: Array.isArray(analysisData.brandElements) ? analysisData.brandElements : [],
        composition: analysisData.composition || {
          shotType: 'medium',
          cameraMovement: 'static',
          cameraAngle: 'eye-level',
          lighting: 'natural',
          depth: 'medium'
        },
        transitionIn: analysisData.transitionIn,
        transitionOut: analysisData.transitionOut,
        motionIntensity: typeof analysisData.motionIntensity === 'number' ? analysisData.motionIntensity : 5,
        visualComplexity: typeof analysisData.visualComplexity === 'number' ? analysisData.visualComplexity : 5,
        visualScore: typeof analysisData.visualScore === 'number' ? analysisData.visualScore : 7,
        engagementScore: typeof analysisData.engagementScore === 'number' ? analysisData.engagementScore : 6,
        keyframes: scene.keyframes.map(kf => ({
          timestamp: kf.timestamp,
          url: kf.url,
          description: `Keyframe em ${kf.timestamp.toFixed(2)}s`
        }))
      };

    } catch (error) {
      console.error(`‚ùå Technical scene analysis failed for scene ${scene.id}:`, error);
      throw error;
    }
  }

  /**
   * Create fallback analysis when GPT-4o analysis fails
   */
  private createFallbackSceneAnalysis(scene: {
    id: number;
    startSec: number;
    endSec: number;
    durationSec: number;
    keyframes: Array<{ timestamp: number; url: string }>;
  }): any {
    return {
      id: scene.id,
      startSec: scene.startSec,
      endSec: scene.endSec,
      durationSec: scene.durationSec,
      technicalDescription: `Cena ${scene.id} (${scene.startSec.toFixed(2)}s-${scene.endSec.toFixed(2)}s): An√°lise t√©cnica n√£o dispon√≠vel devido a erro no processamento. Cena com dura√ß√£o de ${scene.durationSec.toFixed(2)} segundos contendo ${scene.keyframes.length} keyframes.`,
      objects: [],
      text: [],
      peopleCount: 0,
      dominantColors: [],
      brandElements: [],
      composition: {
        shotType: 'medium',
        cameraMovement: 'static',
        cameraAngle: 'eye-level',
        lighting: 'natural',
        depth: 'medium'
      },
      motionIntensity: 3,
      visualComplexity: 3,
      visualScore: 3,
      engagementScore: 3,
      keyframes: scene.keyframes.map(kf => ({
        timestamp: kf.timestamp,
        url: kf.url,
        description: `Keyframe em ${kf.timestamp.toFixed(2)}s (an√°lise indispon√≠vel)`
      }))
    };
  }

  /**
   * Calculate cost for scene-based analysis
   */
  calculateSceneAnalysisCost(sceneCount: number, avgKeyframesPerScene: number): number {
    // More complex analysis with multiple images per scene
    const avgTokensPerScene = 1200 + (avgKeyframesPerScene * 600); // Higher token usage for detailed analysis
    const totalInputTokens = sceneCount * avgTokensPerScene;
    const totalOutputTokens = sceneCount * 400; // More detailed output per scene
    
    const inputCost = (totalInputTokens / 1000) * this.modelPricing['gpt-4o'].input;
    const outputCost = (totalOutputTokens / 1000) * this.modelPricing['gpt-4o'].output;
    
    return inputCost + outputCost;
  }

  /**
   * Compare two sets of keyframes (for A/B testing)
   */
  async compareVisualSets(
    setA: Array<{ url: string; timestamp: number }>,
    setB: Array<{ url: string; timestamp: number }>
  ): Promise<{
    winner: 'A' | 'B' | 'tie';
    comparison: {
      visualQuality: { A: number; B: number };
      clarity: { A: number; B: number };
      engagement: { A: number; B: number };
    };
    recommendations: string[];
  }> {
    // Implementation would compare visual elements between two creative sets
    // This is a placeholder for the comparison logic
    return {
      winner: 'tie',
      comparison: {
        visualQuality: { A: 7.5, B: 7.3 },
        clarity: { A: 8.0, B: 7.8 },
        engagement: { A: 6.9, B: 7.1 }
      },
      recommendations: [
        'Set A tem melhor claridade visual',
        'Set B mostra melhor engajamento',
        'Considere combinar elementos dos dois'
      ]
    };
  }
}